# NGHIÃŠN Cá»¨U VÃ€ XÃ‚Y Dá»°NG Há»† THá»NG TRá»°C QUAN HÃ“A BÃ€I THUYáº¾T TRÃŒNH THEO THá»œI GIAN THá»°C Dá»°A TRÃŠN GIá»ŒNG NÃ“I VÃ€ GENERATIVE AI

**Development of a Real-time Voice-to-Slide Presentation System using Generative AI with Streaming UI and Voice-based Dual-Mode Interaction**

---

**Sinh viÃªn thá»±c hiá»‡n:** [TÃªn cá»§a báº¡n]  
**MSSV:** [MÃ£ sá»‘ sinh viÃªn]  
**Giáº£ng viÃªn hÆ°á»›ng dáº«n:** [TÃªn giáº£ng viÃªn]  
**Khoa/Bá»™ mÃ´n:** [TÃªn khoa]  
**NgÃ y ná»™p:** [NgÃ y/ThÃ¡ng/NÄƒm]

---

## Má»¤C Lá»¤C

1. [Giá»›i thiá»‡u](#1-giá»›i-thiá»‡u)
2. [Tá»•ng quan tÃ i liá»‡u](#2-tá»•ng-quan-tÃ i-liá»‡u)
3. [PhÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u](#3-phÆ°Æ¡ng-phÃ¡p-nghiÃªn-cá»©u)
4. [Káº¿ hoáº¡ch Ä‘Ã¡nh giÃ¡](#4-káº¿-hoáº¡ch-Ä‘Ã¡nh-giÃ¡)
5. [Káº¿t quáº£ dá»± kiáº¿n](#5-káº¿t-quáº£-dá»±-kiáº¿n)
6. [Timeline & Milestones](#6-timeline--milestones)
7. [Rá»§i ro & Giáº£i phÃ¡p](#7-rá»§i-ro--giáº£i-phÃ¡p)
8. [ÄÃ³ng gÃ³p khoa há»c](#8-Ä‘Ã³ng-gÃ³p-khoa-há»c)
9. [Háº¡n cháº¿ & HÆ°á»›ng phÃ¡t triá»ƒn](#9-háº¡n-cháº¿--hÆ°á»›ng-phÃ¡t-triá»ƒn)
10. [Káº¿t luáº­n](#10-káº¿t-luáº­n)
11. [TÃ i liá»‡u tham kháº£o](#11-tÃ i-liá»‡u-tham-kháº£o)

---

## 1. GIá»šI THIá»†U

### 1.1. Äáº·t váº¥n Ä‘á»

Trong ká»· nguyÃªn sá»‘ hÃ³a, viá»‡c thuyáº¿t trÃ¬nh Ä‘Ã³ng vai trÃ² then chá»‘t trong giÃ¡o dá»¥c, kinh doanh vÃ  nghiÃªn cá»©u. Tuy nhiÃªn, quy trÃ¬nh táº¡o slide thuyáº¿t trÃ¬nh truyá»n thá»‘ng (PowerPoint, Google Slides) tá»“n táº¡i nhiá»u háº¡n cháº¿ nghiÃªm trá»ng:

#### Váº¥n Ä‘á» 1: GiÃ¡n Ä‘oáº¡n tÆ° duy (Cognitive Flow Interruption)

Khi soáº¡n slide thá»§ cÃ´ng, ngÆ°á»i dÃ¹ng pháº£i:
- Chuyá»ƒn Ä‘á»•i liÃªn tá»¥c giá»¯a "suy nghÄ© ná»™i dung" vÃ  "thiáº¿t káº¿ giao diá»‡n"
- Dá»«ng dÃ²ng cháº£y tÆ° duy Ä‘á»ƒ thao tÃ¡c chuá»™t, bÃ n phÃ­m
- Máº¥t focus vÃ o formatting thay vÃ¬ content

**Háº­u quáº£:** 
- Cháº¥t lÆ°á»£ng ná»™i dung giáº£m
- Thá»i gian tÄƒng (trung bÃ¬nh 3-5 phÃºt/slide)
- Stress vÃ  má»‡t má»i

#### Váº¥n Ä‘á» 2: CÃ´ng cá»¥ AI thiáº¿u tÃ­nh tÆ°Æ¡ng tÃ¡c thá»i gian thá»±c

CÃ¡c cÃ´ng cá»¥ AI hiá»‡n nay (Gamma.ai, Tome.app, SlidesAI) hoáº¡t Ä‘á»™ng theo **"Batch Processing"**:

```
User nháº­p Ä‘áº§y Ä‘á»§ content â†’ Chá» Ä‘á»£i xá»­ lÃ½ â†’ Nháº­n káº¿t quáº£ hoÃ n chá»‰nh
```

**Háº¡n cháº¿:**
- âŒ KhÃ´ng phÃ¹ há»£p vá»›i **brainstorming** (lÃªn Ã½ tÆ°á»Ÿng tá»± nhiÃªn)
- âŒ KhÃ´ng cho phÃ©p **Ä‘iá»u chá»‰nh trong quÃ¡ trÃ¬nh** táº¡o
- âŒ Thiáº¿u **feedback tá»©c thÃ¬** â†’ KhÃ´ng biáº¿t system cÃ³ hiá»ƒu Ä‘Ãºng khÃ´ng
- âŒ **Pháº£i chuáº©n bá»‹ content xong** má»›i báº¯t Ä‘áº§u â†’ Máº¥t tÃ­nh tá»± phÃ¡t

#### Váº¥n Ä‘á» 3: Thiáº¿u nghiÃªn cá»©u vá» Voice-First Authoring

Hiá»‡n nay chÆ°a cÃ³ nhiá»u nghiÃªn cá»©u há»c thuáº­t vá»:
- **Real-time streaming** tá»« speech sang visual content
- **Voice-only interaction** (100% giá»ng nÃ³i, khÃ´ng keyboard/mouse)
- **Incremental rendering** (ná»™i dung xuáº¥t hiá»‡n dáº§n, khÃ´ng chá» Ä‘á»£i)
- **Dual-mode voice interaction** (sÃ¡ng táº¡o vs chá»‰nh sá»­a)

**Research Gap:** CÃ¡c nghiÃªn cá»©u hiá»‡n cÃ³ (PASS, PresentAgent) focus vÃ o document-to-slides, **KHÃ”NG cÃ³** há»‡ thá»‘ng speech-to-slides real-time.

---

### 1.2. Nhu cáº§u thá»±c táº¿

#### Use Cases

**ğŸ‘¨â€ğŸ« GiÃ¡o dá»¥c:**
- GiÃ¡o viÃªn chuáº©n bá»‹ bÃ i giáº£ng tá»« ghi chÃº nhanh
- Sinh viÃªn táº¡o slide thuyáº¿t trÃ¬nh Ä‘á»“ Ã¡n
- Gia sÆ° chuáº©n bá»‹ tÃ i liá»‡u giáº£ng dáº¡y

**ğŸ’¼ Doanh nghiá»‡p:**
- Brainstorming session â†’ Visual slides ngay láº­p tá»©c
- Sales pitch creation trong meeting
- Quick deck cho client presentation

**ğŸ“Š NghiÃªn cá»©u:**
- Researcher chuáº©n bá»‹ conference talk
- Lab meeting slides tá»« experiment notes
- Visual aids cho paper presentation

**ğŸ¤ Diá»…n giáº£:**
- Improvised presentations
- Rapid prototyping cá»§a talk ideas
- On-the-fly slide creation during Q&A

**â™¿ Accessibility:**
- NgÆ°á»i khuyáº¿t táº­t tay/ngÃ³n tay
- Repetitive strain injury (RSI) sufferers
- Visual impairment (voice-first = eyes-free)

#### Äáº·c Ä‘iá»ƒm lÃ½ tÆ°á»Ÿng cá»§a cÃ´ng cá»¥

Má»™t cÃ´ng cá»¥ lÃ½ tÆ°á»Ÿng cáº§n:
1. âš¡ **Pháº£n há»“i tá»©c thÃ¬**: Ná»™i dung xuáº¥t hiá»‡n < 2 giÃ¢y
2. ğŸ¬ **Streaming UI**: Content "má»c lÃªn" dáº§n (nhÆ° Gamma.app)
3. ğŸ—£ï¸ **Voice-only**: 100% giá»ng nÃ³i, khÃ´ng cáº§n tay
4. ğŸ¯ **Dual-mode**: SÃ¡ng táº¡o tá»± nhiÃªn + Chá»‰nh sá»­a chÃ­nh xÃ¡c
5. ğŸ“Š **Cháº¥t lÆ°á»£ng cao**: Comparable vá»›i AI tools hiá»‡n cÃ³

â†’ **GenSlide** Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Ã¡p á»©ng táº¥t cáº£ nhu cáº§u trÃªn.

---

### 1.3. Má»¥c tiÃªu nghiÃªn cá»©u

Äá» tÃ i nháº±m xÃ¢y dá»±ng há»‡ thá»‘ng pháº§n má»m (Web Application) vá»›i cÃ¡c má»¥c tiÃªu cá»¥ thá»ƒ:

#### Má»¥c tiÃªu 1: Real-time Streaming Architecture
- **Speech Layer**: Streaming ASR vá»›i two-pass decoding (preview 300ms + accurate 1.5s)
- **Language Layer**: Streaming LLM response token-by-token
- **Render Layer**: Incremental DOM updates vá»›i animations
- **Target latency**: < 2 seconds end-to-end

#### Má»¥c tiÃªu 2: Dual-Mode Voice Interaction
- **Brainstorming Mode**: 
  - User nÃ³i tá»± nhiÃªn: "HÃ´m nay tÃ´i muá»‘n nÃ³i vá» AI trong y táº¿..."
  - System tá»± hiá»ƒu vÃ  táº¡o slide structure
  - No explicit commands needed
  
- **Editing Mode**:
  - User ra lá»‡nh: "Sá»­a tiÃªu Ä‘á» thÃ nh...", "XÃ³a bullet thá»© 2"
  - System parse commands vÃ  execute
  - Context-aware ("cÃ¡i Ä‘Ã³", "slide nÃ y")

- **Mode Switching**: MÆ°á»£t mÃ  báº±ng button click

#### Má»¥c tiÃªu 3: Streaming UI Experience
- Ná»™i dung xuáº¥t hiá»‡n **dáº§n dáº§n** (Title â†’ Bullet 1 â†’ Bullet 2...)
- **KhÃ´ng** chá» Ä‘á»£i rá»“i "báº­t lÃªn" (wait-then-pop)
- Animations: Typewriter effect, Fade-in, Slide-up
- Maintain 60 FPS performance

#### Má»¥c tiÃªu 4: Quality Maintenance
- PresentEval Content Fidelity: > 8.0/10
- Visual Clarity: > 7.5/10
- Intent Classification: > 90% accuracy (Brainstorm), > 85% (Edit)
- Comparable with SOTA (PASS: 9.02/10) - acceptable 10% gap

#### Má»¥c tiÃªu 5: ÄÃ³ng gÃ³p khoa há»c
- First speech-to-slide real-time system
- Incremental JSON parsing algorithm
- Dual-mode voice interaction framework
- Benchmark protocol using SlideSpeech dataset
- Optimal update frequency guidelines (evidence-based)

---

### 1.4. CÃ¢u há»i nghiÃªn cá»©u

#### RQ1: Real-time Speech-to-Slide Streaming Alignment â­â­â­
> **"LÃ m tháº¿ nÃ o Ä‘á»“ng bá»™ hÃ³a continuous speech stream vá»›i incremental slide rendering Ä‘á»ƒ táº¡o cáº£m giÃ¡c 'Ä‘ang Ä‘Æ°á»£c táº¡o ra' thay vÃ¬ 'Ä‘á»£i xong xuáº¥t hiá»‡n'?"**

**Techniques cáº§n nghiÃªn cá»©u:**
- **Sliding Window ASR**: Window size optimal? 640ms tá»« U2 Whisper paper?
- **Two-Pass Decoding**: CTC (fast preview) + Attention (accurate final)
- **VAD with Prosody**: Káº¿t há»£p acoustic + prosody + linguistic signals
- **Word-level Timestamps**: Forced alignment cho emphasis detection

**Metrics:**
- Time to First Content (TTFC): Target < 1.5s
- Update Frequency: 5-10 updates/second
- Perceived Responsiveness: Likert 1-7 scale

**Baseline:**
- Batch mode: TTFC = 3s, 1 update total
- Streaming target: TTFC < 1.5s, 5-10 updates

**Hypothesis:** Streaming approach vá»›i two-pass decoding sáº½ achieve TTFC < 2s while maintaining accuracy (WER < 10%).

---

#### RQ2: Incremental JSON Parsing for Streaming LLM â­â­â­
> **"LÃ m tháº¿ nÃ o parse vÃ  render JSON structure khi nÃ³ Ä‘ang Ä‘Æ°á»£c generate token-by-token tá»« LLM streaming API?"**

**Research Gap:**
- Existing JSON parsers: Require complete, valid JSON
- LLM streaming: Outputs tokens sequentially
- Need: Parser that works on **partial** JSON

**Techniques cáº§n phÃ¡t triá»ƒn:**
- **State Machine Parser**: Track parsing state (IN_TITLE, IN_BULLETS, ...)
- **Field-level Granularity**: CÃ³ thá»ƒ render title trÆ°á»›c khi bullets ready
- **Error Recovery**: Handle invalid partial JSON gracefully
- **Buffering Strategy**: Parse every token? Every N tokens? On complete field?

**Metrics:**
- Parse success rate on partial JSONs: Target > 95%
- Latency overhead: < 10ms per token
- False positive renders: < 5% (render incomplete/wrong content)

**Novel Contribution:**
- First application of streaming JSON parsing trong interactive UI generation
- Prior work: Batch JSON parsing (PASS, PresentAgent)
- Open-source library Ä‘á»ƒ release

**Example:**
```
Token stream: '{"title":"AI' â†’ Partial: {title: "AI"}
              â†’ Renderable? YES! â†’ UI updates
              
Token stream: ' trong Y táº¿",' â†’ Complete: {title: "AI trong Y táº¿"}
              â†’ Renderable? YES! â†’ UI updates again
```

---

#### RQ3: Incremental DOM Update Optimization â­â­
> **"Chiáº¿n lÆ°á»£c nÃ o cho incremental DOM updates mang láº¡i tráº£i nghiá»‡m smooth nháº¥t mÃ  khÃ´ng gÃ¢y flicker hay performance bottleneck?"**

**Approaches cáº§n so sÃ¡nh:**
- **A**: Full re-render (baseline)
- **B**: Virtual DOM diffing (React-style)
- **C**: Direct DOM + requestAnimationFrame batching
- **D**: Morphdom library

**Metrics:**
- **Frame Rate**: Target maintain 60 FPS
- **Layout Thrashing**: Count forced reflows (target < 2 per update)
- **Memory Overhead**: Virtual DOM memory usage
- **Perceived Smoothness**: User rating (Likert 1-7)

**Hypothesis:**
- Virtual DOM (B) hoáº·c RAF batching (C) sáº½ significantly outperform full re-render (p < 0.01)
- Trade-off: B cÃ³ memory overhead nhÆ°ng smoother, C Ã­t memory nhÆ°ng cáº§n careful coordination

**Experiment:**
- Same content, different rendering strategies
- Measure FPS, layout thrashing, user preference
- Statistical test: ANOVA + post-hoc

---

#### RQ4: Dual-Mode Voice Interaction Design â­â­â­
> **"LÃ m tháº¿ nÃ o thiáº¿t káº¿ interaction pattern cho phÃ©p user seamlessly switch giá»¯a Brainstorming Mode (generative) vÃ  Editing Mode (imperative) chá»‰ báº±ng giá»ng nÃ³i?"**

**Research Gap:**
- PASS/PresentAgent: One-way (document â†’ slides), no editing
- Gamma.ai: Manual editing only (keyboard/mouse)
- Voice assistants (Siri, Alexa): Single mode (command-based)
- **GenSlide**: Dual-mode, voice-only - **FIRST!**

**Challenges:**
1. **Intent Classification Ambiguity**
   - Brainstorm: "ThÃªm Ã½ vá» AI" â†’ Generate content about AI
   - Edit: "ThÃªm Ã½ vá» AI" â†’ Execute command "add bullet mentioning AI"
   - Same words, different meanings!

2. **Context Management**
   - "Sá»­a cÃ¡i Ä‘Ã³" â†’ Which element?
   - "XÃ³a Ã½ thá»© 2" â†’ Which slide? Current or previous?
   - Need context tracking

3. **Mode Switching UX**
   - How does user know current mode?
   - Visual indicators sufficient?
   - Accidental mode switches?

**Techniques:**
- **Separate LLM Prompts** per mode
- **Context Manager**: Track current slide, last mentioned element
- **Reference Resolution**: NLP for "cÃ¡i Ä‘Ã³", "Ã½ thá»© 2"
- **Visual Feedback**: Clear mode indicators

**User Study Scenarios:**
1. Brainstorm 3 slides â†’ Switch to Edit â†’ Modify slide 2 â†’ Resume brainstorm
2. Complex edit: "Sá»­a bullet thá»© 2 cá»§a slide Ä‘áº§u tiÃªn thÃ nh..."
3. Ambiguous reference: "XÃ³a cÃ¡i Ä‘Ã³" â†’ System should ask for clarification?

**Metrics:**
- Intent classification accuracy: Brainstorm mode > 90%, Edit mode > 85%
- Command execution success rate: > 90%
- Mode confusion rate: < 10% (user doesn't know current mode)
- User satisfaction vá»›i mode switching: Likert > 5/7

---

#### RQ5: Optimal Streaming Update Frequency â­â­
> **"Táº§n suáº¥t cáº­p nháº­t UI bao nhiÃªu láº§n/giÃ¢y lÃ  tá»‘i Æ°u cho readability vÃ  perceived responsiveness?"**

**Hypothesis:**
Inverted U-shape relationship:
- Too slow (< 3 Hz): Choppy, feels laggy
- Too fast (> 15 Hz): Overwhelming, hard to read
- **Sweet spot: 6-10 Hz** (based on human perception research)

**Experiment Design:**
- **Type**: Within-subjects
- **Participants**: n=20
- **Conditions**: 6 levels (1Hz, 3Hz, 6Hz, 10Hz, 15Hz, 30Hz)
- **Same content**, different update rates
- **Randomized order** to control for learning effects

**Measurements:**
- **Reading Comprehension**: Recall test after each condition
- **Perceived Smoothness**: Likert 1-7 ("How smooth was the animation?")
- **Preference Ranking**: Rank conditions from best to worst

**Statistical Analysis:**
- One-way repeated measures ANOVA
- Post-hoc: Tukey HSD for pairwise comparisons
- Effect size: Partial eta-squared
- Expected: Main effect of frequency (p < 0.001), peak at 8-10Hz

**Practical Contribution:**
Evidence-based guideline cho streaming UI design, generalizable beyond presentations.

---

#### RQ6: Streaming vs Batch Quality Trade-off â­â­
> **"CÃ³ sá»± trade-off nÃ o giá»¯a streaming mode (real-time) vÃ  batch mode (wait for complete) vá» cháº¥t lÆ°á»£ng ná»™i dung khÃ´ng?"**

**Hypothesis:**
- **H0** (null): No significant difference in quality
- **H1**: Streaming < Batch (vÃ¬ LLM khÃ´ng "think" Ä‘á»§ before outputting)

**Rationale for H1:**
- Streaming: LLM outputs token-by-token â†’ Less "thinking time"
- Batch: LLM sees full input, can plan complete response
- Trade-off: Speed vs Quality?

**Experiment:**
- **Same 100 speech inputs** (from SlideSpeech test set)
- **Condition A**: Streaming (Gemini stream=True)
- **Condition B**: Batch (Gemini stream=False)
- **Condition C**: Streaming + Polish pass (hypothesis: eliminates gap)
- **Evaluate**: PresentEval framework (Content Fidelity, Visual Clarity)

**Metrics:**
- Content Fidelity: 0-10 scale (VLM evaluation)
- Grammar errors count
- Coherence score
- User preference (blind A/B test)

**Expected Results:**
```
Condition  | Fidelity | Latency | User Pref
-----------|----------|---------|----------
A (Stream) | 7.8      | 2s      | 30%
B (Batch)  | 8.3      | 5s      | 20%
C (S+Polish)| 8.2     | 3s      | 50% â† BEST!
```

**Contribution:**
- Demonstrate polish pass as effective mitigation
- Quantify acceptable quality gap for speed gain
- Inform design decisions for real-time AI systems

---

## 2. Tá»”NG QUAN TÃ€I LIá»†U

### 2.1. CÃ´ng cá»¥ hiá»‡n cÃ³

#### 2.1.1. CÃ´ng cá»¥ truyá»n thá»‘ng

| Tool | Strengths | Limitations | Time/Slide |
|------|-----------|-------------|------------|
| PowerPoint | Full control, powerful features | Manual, steep learning curve | 3-5 min |
| Google Slides | Collaboration, cloud-based | Similar manual process | 3-5 min |
| Keynote | Beautiful design templates | macOS only, manual | 4-6 min |

**TÃ³m táº¯t:** Cháº¥t lÆ°á»£ng cao nhÆ°ng **tá»‘n thá»i gian** vÃ  **giÃ¡n Ä‘oáº¡n tÆ° duy**.

#### 2.1.2. CÃ´ng cá»¥ AI hiá»‡n táº¡i

**Gamma.ai:**
- âœ… Beautiful UI vá»›i streaming rendering effect
- âœ… AI-powered content generation
- âŒ **Batch input**: Paste text Ä‘áº§y Ä‘á»§, khÃ´ng real-time
- âŒ **No voice**: Keyboard/mouse only
- âŒ **Manual editing**: Click vÃ  type

**Tome.app:**
- âœ… AI content generation
- âœ… Multimodal (text + images)
- âŒ Batch processing
- âŒ No voice interaction
- âŒ Wait for complete deck

**SlidesAI:**
- âœ… Simple, fast
- âŒ Limited customization
- âŒ Batch, no streaming
- âŒ No voice

**Beautiful.ai:**
- âœ… Smart templates, auto-layout
- âŒ Manual input only
- âŒ Focus on design, not speed

### 2.2. State-of-the-Art Academic Research (9 Papers)

#### Paper 1: PASS (2025) â­â­â­
**Liu et al. "Presentation Automation for Slide Generation and Speech"**

**Key Contributions:**
- Modular pipeline: Document â†’ Titles â†’ Content â†’ Refinement â†’ Script â†’ TTS
- Multi-model comparison: GPT-4o-PASS (9.02/10) >> D2S fine-tuned (7.34/10)
- **PresentEval framework**: Content Fidelity, Visual Clarity, Coherence

**Relevance to GenSlide:**
- âœ… Modular architecture inspiring
- âœ… PresentEval metrics directly applicable
- âœ… Content generation techniques
- âŒ Direction: **Document** â†’ Slides (GenSlide: **Speech** â†’ Slides)
- âŒ Batch processing, not real-time
- âŒ No voice interaction

**GenSlide will adapt:**
- Modular pipeline design
- PresentEval for evaluation
- Reverse pipeline direction

---

#### Paper 2: PresentAgent (2025) â­â­â­
**Zhang et al. "Multimodal Agent for Presentation Video Generation"**

**Key Contributions:**
- **Multi-LLM ensemble** (GPT-4o + Gemini + Claude) > Single model
- Comprehensive PresentEval implementation
- Prosody-aware TTS
- Layout-aware slide composition

**Relevance to GenSlide:**
- âœ… **CRITICAL**: Multi-model routing strategy
- âœ… PresentEval framework detailed
- âœ… Multimodal generation approach
- âŒ Document-driven, not voice
- âŒ Batch processing
- âŒ Output: Video, not interactive slides

**GenSlide will adopt:**
- Multi-LLM routing (Gemini Fast vs GPT-4o vs Claude)
- PresentEval evaluation
- Layout generation techniques

---

#### Paper 3: U2 Whisper (2025) â­â­â­
**Li et al. "Adapting Whisper for Streaming ASR via Two-Pass Decoding"**

**Key Contributions:**
- Two-pass: CTC branch (200ms fast) + Attention branch (1.5s accurate)
- Streaming-friendly architecture
- Real-time factor < 1 on CPU
- Optimal chunk size: 640ms

**Relevance to GenSlide:**
- âœ… **ESSENTIAL** - Solves Whisper's streaming limitation
- âœ… Two-pass perfect for preview + final
- âœ… Low latency enables < 2s target
- âŒ English/Mandarin focus (need adapt for Vietnamese)

**GenSlide will use:**
- Two-pass decoding: Preview for UI, Final for LLM
- 640ms chunk size
- Benchmark on Vietnamese presentation domain

---

#### Paper 4: WhisperX (2023) â­â­â­
**Bain et al. "Word-level Timestamps & Diarization"**

**Key Contributions:**
- Forced alignment vá»›i Wav2Vec2 â†’ Word-level timestamps
- 70x realtime speed vá»›i batching
- No WER degradation with VAD preprocessing

**Relevance to GenSlide:**
- âœ… **CRITICAL** - Word timestamps cho emphasis detection
- âœ… 70x speed enables real-time
- âœ… Production-ready tool
- âœ… Open-source, actively maintained

**GenSlide will use:**
- Word timestamps to detect emphasized words (speak slowly = important â†’ bold)
- Align speech segments vá»›i slide content
- VAD for sentence boundary detection

---

#### Paper 5: SlideSpeech (2023) â­â­â­
**Xu et al. "Large-Scale Slide-Enriched Audio-Visual Corpus"**

**Key Contributions:**
- 1,705 videos, 1,000+ hours synchronized speech-slides
- 22 domains (CS, music, history, agriculture, ...)
- Benchmark dataset cho multimodal ASR

**Relevance to GenSlide:**
- âœ… **THE DATASET** - 1000+ hours perfect for evaluation
- âœ… Synchronized speech-slides = ground truth
- âœ… Cross-domain coverage
- âŒ Designed for ASR, not generation

**GenSlide will use:**
- **Evaluation benchmark** (test set: 25 videos, 8.75h)
- Learn slide patterns from training data
- Baseline comparison vá»›i human-created slides

---

#### Paper 6: Few-shot Style Transfer (2022) â­â­
**Krishnan et al. "Few-shot Style Transfer for Multilingual Settings"**

**Key Contributions:**
- Style transfer vá»›i chá»‰ 3-10 examples (no large corpus needed)
- Paraphrase-based style modeling
- Controllable magnitude (0-1 scalar)

**Relevance to GenSlide:**
- âœ… Personalization vá»›i minimal examples
- âœ… No retraining needed
- âŒ Text style only, not visual/layout

**GenSlide future extension:**
- Learn user's slide style tá»« 5-10 examples
- Apply style preferences automatically
- Controllable style strength

---

#### Paper 7-9: Supporting Papers â­

**LayoutLMv3** (Huang et al. 2022):
- Joint text-layout understanding
- Could analyze user's example slides

**PosterLLaVa** (Li et al. 2024):
- LLM generates layouts (bounding boxes)
- Content-aware approach

**Auto-Slides** (Chen et al. 2023):
- Multi-agent collaboration (Parser, Verifier, Repair)
- Interactive refinement

**GenSlide potential use:**
- Layout understanding from examples
- LLM-based layout generation
- Quality verification-repair loop

---

### 2.3. Research Gap - Báº£ng so sÃ¡nh

| Aspect | Existing Work | **GenSlide Contribution** |
|--------|---------------|---------------------------|
| **Input** | Document (PASS, PresentAgent) | **Speech (real-time)** |
| **Processing** | Batch (wait for complete) | **Streaming (continuous)** |
| **Interaction** | One-way (input â†’ output) | **Dual-mode (Brainstorm â†” Edit)** |
| **UI** | Static (wait then appear) | **Streaming UI (gradual)** |
| **Latency** | 30s - 2 minutes | **< 2 seconds** |
| **Editing** | Manual (keyboard/mouse) or No editing | **Voice-only commands** |
| **Modality** | Text/Document input | **Voice-first, voice-only** |
| **Evaluation** | Document-based | **Speech-to-slide (new)** |

**Key Novel Contributions:**

1. âœ… First real-time speech-to-slide system vá»›i streaming architecture
2. âœ… First voice-only dual-mode interaction (Brainstorm vs Edit)
3. âœ… First application of streaming LLM cho structured data generation
4. âœ… Incremental JSON parsing algorithm
5. âœ… Benchmark protocol for speech-to-slide using SlideSpeech
6. âœ… Evidence-based optimal update frequency guidelines

---

## 3. PHÆ¯Æ NG PHÃP NGHIÃŠN Cá»¨U

### 3.1. PhÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n

**Research Paradigm:** Design Science Research + Experimental Evaluation

**Quy trÃ¬nh 4 bÆ°á»›c:**

1. **Design & Build** (Weeks 1-6):
   - Thiáº¿t káº¿ streaming architecture
   - Implement prototype vá»›i core features
   - Iterative development + testing

2. **Demonstrate** (Week 7):
   - Demo scenarios covering key use cases
   - Pilot testing vá»›i 5 users
   - Identify major bugs

3. **Evaluate** (Weeks 8-11):
   - Benchmark evaluation (SlideSpeech)
   - User study (n=20)
   - Statistical analysis

4. **Communicate** (Week 12):
   - Write research paper
   - Prepare presentation
   - Open-source release

---

### 3.2. Kiáº¿n trÃºc há»‡ thá»‘ng

#### 3.2.1. System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GENSLIDE ARCHITECTURE                           â”‚
â”‚                   (True Real-time Streaming)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   CONTINUOUS    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   CONTINUOUS    â”‚
â”‚  â”‚   SPEECH     â”‚â•â•â• STREAM 1 â•â•â•â–¶â”‚   LANGUAGE   â”‚â•â•â• STREAM 2 â•â•â•â–¶â”‚
â”‚  â”‚   LAYER      â”‚                  â”‚   LAYER      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚       â•‘                                   â•‘                          â”‚
â”‚       â•‘ Partial Transcripts               â•‘ Token-by-Token           â”‚
â”‚       â•‘ (every 200-300ms)                 â•‘ JSON Fragments           â”‚
â”‚       â–¼                                   â–¼                          â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   RENDER     â”‚â—€â•â•â• STREAM 3 â•â•â•â•â”‚    STATE     â”‚                 â”‚
â”‚  â”‚   LAYER      â”‚                  â”‚   MANAGER    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚       â”‚                                   â”‚                          â”‚
â”‚       â”‚ Incremental DOM Updates           â”‚ Mode Switching           â”‚
â”‚       â”‚ (Title â†’ Bullets â†’ Polish)        â”‚ (Brainstorm â†” Edit)     â”‚
â”‚       â–¼                                   â–¼                          â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚       USER SEES CONTENT APPEARING GRADUALLY                  â”‚   â”‚
â”‚  â”‚   (Like watching someone type, NOT waiting for load)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY FEATURES:
â•â•â• = Continuous streaming (khÃ´ng pháº£i wait-then-process)
STREAM 1-3 = Three concurrent data streams
STATE MANAGER = Mode switching logic (Brainstorm â†” Edit)
```

**Giáº£i thÃ­ch:**
- **Double lines (â•â•â•)**: DÃ²ng cháº£y liÃªn tá»¥c, khÃ´ng pháº£i single arrow
- **3 Streams cháº¡y song song**: Speech, Language, Render overlap nhau
- **State Manager**: Component má»›i Ä‘á»ƒ quáº£n lÃ½ dual-mode
- **Incremental Updates**: Render tá»«ng pháº§n, khÃ´ng Ä‘á»£i full JSON

---

#### 3.2.2. Component 1: Speech Layer (Streaming ASR)

**Má»¥c Ä‘Ã­ch:** Chuyá»ƒn Ä‘á»•i speech thÃ nh text vá»›i latency < 2s vÃ  continuous updates

**Kiáº¿n trÃºc:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        STREAMING SPEECH LAYER                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Microphone (16kHz mono)                    â”‚
â”‚       â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ VAD Module  â”‚ â† Detect speech/silence    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    + sentence boundaries  â”‚
â”‚       â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Sliding Window Buffer           â”‚       â”‚
â”‚  â”‚ â€¢ Window: 640ms                 â”‚       â”‚
â”‚  â”‚ â€¢ Overlap: 160ms (25%)          â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Two-Pass Decoder                â”‚       â”‚
â”‚  â”‚  Pass 1: CTC Branch             â”‚       â”‚
â”‚  â”‚   â†’ Fast preview (300ms)        â”‚       â”‚
â”‚  â”‚  Pass 2: Attention Branch       â”‚       â”‚
â”‚  â”‚   â†’ Accurate final (1.5s)       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â†“              â†“                      â”‚
â”‚  Preview Text    Final + Timestamps        â”‚
â”‚       â†“              â†“                      â”‚
â”‚  To UI           To LLM                     â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ká»¹ thuáº­t chÃ­nh:**

**A. Sliding Window ASR**
- Process audio theo chunks 640ms (optimal tá»« U2 Whisper paper)
- Overlap 25% Ä‘á»ƒ trÃ¡nh máº¥t tá»« á»Ÿ boundary
- KhÃ´ng Ä‘á»£i user nÃ³i xong má»›i xá»­ lÃ½

Example:
```
User nÃ³i: "HÃ´m nay tÃ´i muá»‘n nÃ³i vá» AI trong y táº¿"
         â”œâ”€â”€â”€â”€â”€â”€â”¤ Window 1 (640ms): "HÃ´m nay tÃ´i"
              â”œâ”€â”€â”€â”€â”€â”€â”¤ Window 2: "tÃ´i muá»‘n nÃ³i"
                   â”œâ”€â”€â”€â”€â”€â”€â”¤ Window 3: "nÃ³i vá» AI"
                        â”œâ”€â”€â”€â”€â”€â”€â”¤ Window 4: "AI trong y táº¿"

Má»—i window Ä‘Æ°á»£c xá»­ lÃ½ SONG SONG â†’ Partial results liÃªn tá»¥c
```

**B. Two-Pass Decoding**

**Pass 1 - CTC Branch (Fast Preview):**
- Latency: 200-300ms
- Accuracy: Lower (cÃ³ thá»ƒ thiáº¿u dáº¥u, viáº¿t hoa sai)
- Purpose: Cho user biáº¿t "system Ä‘ang nghe"
- Output: Partial hypothesis
- Display: Preview text (mÃ u xÃ¡m)

**Pass 2 - Attention Branch (Accurate Final):**
- Latency: 1.5-2s (sau khi detect utterance end)
- Accuracy: High (Ä‘Ãºng dáº¥u, viáº¿t hoa, ngá»¯ phÃ¡p)
- Purpose: Quality transcript Ä‘á»ƒ send cho LLM
- Output: Final transcript + word timestamps
- Display: Replace preview â†’ Final text (mÃ u Ä‘en)

**Timeline Example:**
```
t=0.0s:   User starts: "HÃ´m nay..."
t=0.3s:   Pass 1 shows: "hÃ´m nay" (lowercase, no diacritics)
t=0.6s:   Pass 1 updates: "hÃ´m nay tÃ´i"
t=1.0s:   Pass 1 updates: "hÃ´m nay tÃ´i muá»‘n"
t=1.5s:   User pauses
t=2.0s:   VAD detects end
t=3.5s:   Pass 2 finalizes: "HÃ´m nay tÃ´i muá»‘n nÃ³i vá» AI"
                            (Proper capitalization, diacritics)
```

**C. Advanced VAD (Voice Activity Detection)**

KhÃ´ng chá»‰ detect speech vs silence, mÃ  detect **sentence boundaries** báº±ng 3 signals:

1. **Acoustic signal**: Pause duration
   - Pause > 500ms â†’ Likely sentence end (confidence 0.7)

2. **Prosody signal**: Pitch contour
   - Falling pitch â†’ Sentence end (confidence 0.8)
   - Rising pitch â†’ Question or continuation (confidence 0.3)

3. **Linguistic signal**: NLP analysis
   - Complete phrase detection (confidence 0.9)

**Combined confidence score:**
```python
combined = (acoustic * 0.3) + (prosody * 0.3) + (linguistic * 0.4)
if combined > 0.75:
    trigger_pass2_decoding()
```

**D. Word-level Timestamps (WhisperX)**

Sá»­ dá»¥ng forced alignment Ä‘á»ƒ cÃ³ timestamp chÃ­nh xÃ¡c cho tá»«ng tá»«:

```
Output: [
  {word: "AI", start: 2.3, end: 2.6},      # 300ms (normal)
  {word: "ráº¥t", start: 2.6, end: 2.8},     # 200ms (normal)
  {word: "quan", start: 2.8, end: 3.1},    # 300ms (normal)
  {word: "trá»ng", start: 3.1, end: 4.2}    # 1100ms (EMPHASIZED!)
]
```

**Detect emphasis:** Duration > 2x average â†’ Keyword quan trá»ng â†’ Bold in slide

**Example:**
```
User nÃ³i: "AI trong y táº¿ ráº¥t QUAN TRá»ŒNG"
         â”œâ”€ "AI": 2.3-2.6s (300ms - normal)
         â”œâ”€ "trong": 2.6-2.9s (300ms)
         â”œâ”€ "y táº¿": 2.9-3.3s (400ms)
         â””â”€ "QUAN TRá»ŒNG": 3.3-4.5s (1200ms - EMPHASIZED!)

Detection: Duration > 2x average â†’ Make "QUAN TRá»ŒNG" bold in slide
```

**Models:**
- CTC: U2 Whisper CTC branch
- Attention: Whisper large-v3 (fine-tuned on Vietnamese)
- Alignment: Wav2Vec2 Vietnamese
- VAD: WebRTC VAD + Custom prosody analyzer

---

#### 3.2.3. Component 2: Language Layer (Streaming LLM)

**Má»¥c Ä‘Ã­ch:** Convert transcript thÃ nh slide JSON vá»›i streaming output

**Kiáº¿n trÃºc:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STREAMING LANGUAGE LAYER               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                            â”‚
â”‚  Input: Transcript Stream                  â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   State Manager          â”‚             â”‚
â”‚  â”‚   Current Mode:          â”‚             â”‚
â”‚  â”‚   â€¢ BRAINSTORM           â”‚             â”‚
â”‚  â”‚   â€¢ EDIT                 â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Intent Classifier      â”‚             â”‚
â”‚  â”‚   (Different per mode)   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Multi-LLM Router       â”‚             â”‚
â”‚  â”‚   â€¢ Gemini Flash (fast)  â”‚             â”‚
â”‚  â”‚   â€¢ GPT-4o (quality)     â”‚             â”‚
â”‚  â”‚   â€¢ Claude (reasoning)   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Streaming API Call     â”‚             â”‚
â”‚  â”‚   stream=True            â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  Token Stream: '{"title":"AI' ...         â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Incremental JSON Parser  â”‚             â”‚
â”‚  â”‚ (State machine-based)    â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  Partial JSON: {title: "AI"}              â”‚
â”‚       â†“                                    â”‚
â”‚  To Renderer                               â”‚
â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ká»¹ thuáº­t chÃ­nh:**

**A. State Manager - Mode Switching**

```python
class StateManager:
    """
    Quáº£n lÃ½ 2 modes:
    - BRAINSTORM: NÃ³i tá»± nhiÃªn â†’ Táº¡o slide
    - EDIT: Ra lá»‡nh â†’ Chá»‰nh sá»­a slide
    """
    
    def __init__(self):
        self.current_mode = "BRAINSTORM"
        self.current_slide = None
        self.presentation = []
    
    def switch_mode(self, new_mode):
        """Triggered by button click"""
        self.current_mode = new_mode
        self.update_ui_indicator()
        self.update_llm_prompt_template()
```

**B. Dual-Mode Intent Classification**

**BRAINSTORM Mode Prompt:**
```
Báº¡n lÃ  trá»£ lÃ½ GenSlide. User Ä‘ang brainstorm tá»± nhiÃªn.

PhÃ¢n tÃ­ch cÃ¢u nÃ³i vÃ  táº¡o slide JSON:
{
  "intent": "create_slide" | "add_content" | "next_topic",
  "content": {
    "title": "...",
    "bullets": ["...", "..."]
  }
}

Example:
User: "HÃ´m nay tÃ´i muá»‘n nÃ³i vá» AI trong y táº¿"
â†’ {
  "intent": "create_slide",
  "content": {
    "title": "AI trong Y táº¿",
    "bullets": []
  }
}

User: "NÃ³ giÃºp cháº©n Ä‘oÃ¡n bá»‡nh nhanh hÆ¡n"
â†’ {
  "intent": "add_content",
  "content": {
    "bullets": ["Cháº©n Ä‘oÃ¡n bá»‡nh nhanh chÃ³ng"]
  }
}
```

**EDIT Mode Prompt:**
```
Báº¡n lÃ  trá»£ lÃ½ chá»‰nh sá»­a slide. User Ä‘ang ra Lá»†NH.

Parse command vÃ  tráº£ vá» JSON:
{
  "command": "edit_title" | "delete_bullet" | "add_bullet",
  "target": {...},
  "new_value": "..."
}

Example:
User: "Sá»­a tiÃªu Ä‘á» thÃ nh AI vÃ  Machine Learning"
â†’ {
  "command": "edit_title",
  "target": {"slide_index": 0},
  "new_value": "AI vÃ  Machine Learning"
}

User: "XÃ³a bullet thá»© 2"
â†’ {
  "command": "delete_bullet",
  "target": {"slide_index": 0, "bullet_index": 1}
}

Context Awareness:
- "cÃ¡i Ä‘Ã³", "slide nÃ y" â†’ Resolve tá»« context
```

**C. Multi-LLM Router**

```python
def route_to_model(transcript, mode):
    complexity = classify_complexity(transcript)
    
    if mode == "BRAINSTORM":
        if complexity == "simple":  # < 20 words
            return "gemini-flash"  # 200ms latency
        else:
            return "gpt-4o"  # 800ms, better quality
    
    elif mode == "EDIT":
        if complexity == "simple":
            return "gemini-flash"
        elif complexity == "complex":
            return "claude-3.7"  # Best reasoning
```

**D. Streaming LLM API Call**

```python
# KEY: stream=True parameter
response_stream = gemini.generate_content(
    prompt=build_prompt(transcript, mode),
    stream=True,  # â† Enable streaming
    generation_config={
        "temperature": 0.7,
        "max_output_tokens": 1024
    }
)

# Nháº­n tá»«ng token
for chunk in response_stream:
    token = chunk.text
    yield token  # Stream to parser
```

**E. Incremental JSON Parser (CRITICAL!)**

Parse JSON khi nÃ³ Ä‘ang Ä‘Æ°á»£c generate token-by-token:

```python
class IncrementalJSONParser:
    """
    State Machine Parser cho partial JSON
    
    States:
    - INIT: Waiting for '{'
    - IN_TITLE: Parsing title field
    - TITLE_COMPLETE: Title done
    - IN_BULLETS: Parsing bullets array
    - COMPLETE: Full JSON ready
    """
    
    def feed(self, token):
        """
        Feed one token from LLM stream
        Returns: (is_renderable, partial_result)
        """
        self.buffer += token
        
        if self.state == "IN_TITLE":
            if '"' in token:  # Title field closed
                title = self.extract_title(self.buffer)
                self.result['title'] = title
                self.state = "TITLE_COMPLETE"
                return (True, self.result)  # â† Can render!
        
        elif self.state == "IN_BULLETS":
            if '",' in token or '"]' in token:
                bullet = self.extract_bullet(self.buffer)
                self.result['bullets'].append(bullet)
                return (True, self.result)  # â† Can render!
        
        return (False, None)  # Not ready yet
```

**Timeline Example:**

```
t=0.0s: LLM starts: '{'
        State: INIT
        Renderable: No

t=0.6s: LLM: '"title":"AI trong Y táº¿",'
        State: TITLE_COMPLETE
        Result: {title: "AI trong Y táº¿"}
        Renderable: YES! â† UI updates (Title appears)

t=1.3s: LLM: '"bullets":["Cháº©n Ä‘oÃ¡n nhanh",'
        State: BULLET_COMPLETE
        Result: {title: "...", bullets: ["Cháº©n Ä‘oÃ¡n nhanh"]}
        Renderable: YES! â† UI updates (Bullet 1 appears)
```

---

#### 3.2.4. Component 3: Render Layer (Incremental DOM Updates)

**Má»¥c Ä‘Ã­ch:** Update UI mÆ°á»£t mÃ  khi nháº­n partial JSON

**Kiáº¿n trÃºc:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      INCREMENTAL RENDER LAYER               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                            â”‚
â”‚  Input: Partial JSON Stream                â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Virtual DOM Manager      â”‚             â”‚
â”‚  â”‚ (Store previous state)   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Diff Algorithm           â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Animation Controller     â”‚             â”‚
â”‚  â”‚ â€¢ Typewriter effect      â”‚             â”‚
â”‚  â”‚ â€¢ Fade-in animation      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ DOM Patcher              â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚       â†“                                    â”‚
â”‚  Browser (60 FPS)                          â”‚
â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ká»¹ thuáº­t chÃ­nh:**

**A. Virtual DOM Diffing**

```javascript
class IncrementalRenderer {
    render(newSlideJSON) {
        if (!this.previousState) {
            this.fullRender(newSlideJSON);
        } else {
            const diff = this.computeDiff(
                this.previousState, 
                newSlideJSON
            );
            this.applyDiff(diff);
        }
        this.previousState = newSlideJSON;
    }
    
    computeDiff(oldState, newState) {
        return {
            titleChanged: oldState.title !== newState.title,
            titleDelta: newState.title.slice(oldState.title.length),
            bulletsAdded: newState.bullets.slice(oldState.bullets.length)
        };
    }
}
```

**B. Animation Strategies**

**Typewriter Effect:**
```javascript
animateTextAppend(element, newText) {
    let i = 0;
    const interval = setInterval(() => {
        if (i < newText.length) {
            element.textContent += newText[i];
            i++;
        } else {
            clearInterval(interval);
        }
    }, 50);  // 50ms per character
}
```

**Fade-in Effect:**
```javascript
createAndAnimateBullet(text) {
    const bullet = createElement('li');
    bullet.textContent = text;
    bullet.style.opacity = 0;
    
    container.appendChild(bullet);
    
    requestAnimationFrame(() => {
        bullet.style.transition = 'opacity 0.3s';
        bullet.style.opacity = 1;
    });
}
```

---

#### 3.2.5. Component 4: State Manager

**State Diagram:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Click "Edit Mode"
â”‚             â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BRAINSTORM  â”‚                   â”‚
â”‚   MODE      â”‚                   â”‚
â”‚ â€¢ Mic ON    â”‚                   â”‚
â”‚ â€¢ Natural   â”‚ Click "Brainstorm"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   EDIT      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   MODE      â”‚
â”‚ â€¢ Mic ON    â”‚
â”‚ â€¢ Commands  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Transition Logic:**

```python
def on_mode_button_clicked(new_mode):
    if new_mode == "EDIT":
        state_manager.mode = "EDIT"
        ui.show_mode_indicator("EDIT MODE âœï¸")
        llm.prompt_template = EDIT_PROMPT
        
    elif new_mode == "BRAINSTORM":
        state_manager.mode = "BRAINSTORM"
        ui.show_mode_indicator("BRAINSTORM MODE ğŸ¤")
        llm.prompt_template = BRAINSTORM_PROMPT
```

**Context Management:**

```python
class ContextManager:
    def resolve_reference(self, command):
        """
        Resolve "cÃ¡i Ä‘Ã³", "slide nÃ y", "Ã½ thá»© 2"
        """
        if "cÃ¡i Ä‘Ã³" in command:
            return self.last_mentioned_element
        
        if "slide nÃ y" in command:
            return {"slide_index": self.current_slide_index}
        
        match = re.search(r"thá»© (\d+)", command)
        if match:
            index = int(match.group(1)) - 1
            return {"bullet_index": index}
```

---

### 3.3. End-to-End Timeline Example

```
t=0.0s  ğŸ¤ User (Brainstorm): "HÃ´m nay tÃ´i muá»‘n nÃ³i vá» AI"
        
        [Speech Layer]
        - VAD: Speech detected
        - Sliding window starts

t=0.3s  [Speech - Pass 1]
        - Partial: "hÃ´m nay tÃ´i"
        [UI] Preview: "hÃ´m nay tÃ´i"

t=1.5s  [VAD] Sentence end detected
        
        [Speech - Pass 2]
        - Final: "HÃ´m nay tÃ´i muá»‘n nÃ³i vá» AI trong y táº¿."
        
        [Language Layer]
        - Mode: BRAINSTORM
        - Route to: Gemini Flash
        - Stream=True

t=2.1s  [LLM Token Stream]
        - '{"title":"AI trong Y táº¿",'
        
        [Parser]
        - Result: {title: "AI trong Y táº¿"}
        
        [Render]
        - Create title element
        - Typewriter animation
        
        [UI - FIRST CONTENT!]
        - Title appears: "AI trong Y táº¿"
        - Time from speech: 2.1s âœ“

t=3.0s  ğŸ¤ User: "NÃ³ giÃºp cháº©n Ä‘oÃ¡n bá»‡nh"
        
t=4.8s  [LLM generates bullet]
        - Result: {bullets: ["Cháº©n Ä‘oÃ¡n nhanh"]}
        
        [UI]
        - Bullet fades in

t=6.0s  ğŸ‘† User clicks [Edit Mode]
        
        [State Manager]
        - BRAINSTORM â†’ EDIT
        - Update UI: "EDIT MODE âœï¸"

t=7.0s  ğŸ¤ User (Edit): "Sá»­a tiÃªu Ä‘á» thÃ nh AI vÃ  ML"
        
        [Language Layer]
        - Mode: EDIT (different prompt!)
        - Parse: {command: "edit_title", new_value: "..."}
        
        [Render]
        - Update title
        
        [UI]
        - Title changes immediately
```

---

### 3.4. CÃ´ng cá»¥ & CÃ´ng nghá»‡

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Frontend** | Streamlit | Web UI |
| | React | Components |
| **ASR** | WhisperX | Word timestamps |
| | U2 Whisper | Two-pass streaming |
| | Wav2Vec2 | Forced alignment |
| **LLM** | Gemini 2.5 Flash | Fast inference |
| | GPT-4o | Quality |
| | Claude 3.7 | Reasoning |
| **Backend** | Python 3.10+ | Core logic |

---

## 4. Káº¾ HOáº CH ÄÃNH GIÃ

### 4.1. ÄÃ¡nh giÃ¡ Ä‘á»‹nh lÆ°á»£ng (Quantitative Evaluation)

#### 4.1.1. Performance Metrics

**A. Latency Metrics**

| Metric | Definition | Target | Measurement |
|--------|------------|--------|-------------|
| Time to First Content (TTFC) | Speech start â†’ First content xuáº¥t hiá»‡n | < 2s | Timestamp difference |
| Speech-to-Text Latency | Audio end â†’ Final transcript | < 2s | ASR processing time |
| Text-to-JSON Latency | Transcript â†’ Slide JSON | < 1s | LLM processing time |
| JSON-to-Render Latency | JSON â†’ Visual display | < 0.1s | DOM update time |
| End-to-End Latency | Speech start â†’ Polished slide | < 5s | Total pipeline time |

**Benchmark Comparison:**
- Manual PowerPoint: 3-5 minutes per slide
- Gamma.ai: 1-2 minutes per deck (batch)
- **GenSlide target: < 10 seconds per slide (real-time)**

**B. Quality Metrics (PresentEval Framework)**

| Metric | Definition | Method | Target |
|--------|------------|--------|--------|
| Content Fidelity | Slide captures speech accurately | VLM (GPT-4o-mini) | > 8.0/10 |
| Visual Clarity | Layout quality, readability | VLM + Expert | > 7.5/10 |
| Coherence | Logical flow | VLM evaluation | > 7.0/10 |
| Completeness | All key points included | Coverage ratio | > 85% |

**Baseline Comparison:**
- PASS (GPT-4o): 9.02 Â± 0.05
- PresentAgent: ~8.5/10
- **GenSlide target: 8.0-8.5/10** (acceptable 10% gap)

**C. Interaction Metrics**

| Metric | Definition | Target |
|--------|------------|--------|
| Intent Accuracy (Brainstorm) | % correct intent detection | > 90% |
| Intent Accuracy (Edit) | % command parsing | > 85% |
| Mode Switch Success Rate | % successful transitions | > 95% |
| Command Execution Accuracy | % edits executed correctly | > 90% |
| Context Resolution Rate | % references resolved | > 80% |

**D. Streaming UI Metrics**

| Metric | Definition | Target |
|--------|------------|--------|
| Update Frequency | Updates/second | 5-10 Hz |
| Frame Rate | FPS during updates | > 55 FPS |
| Layout Thrashing | Forced reflows | < 2 |
| Perceived Smoothness | User rating (1-7) | > 5.5 |

---

#### 4.1.2. SlideSpeech Benchmark Evaluation

**Dataset:**
- Test set: 25 videos, 8.75 hours
- Domains: CS, biology, history, music

**Protocol:**
1. Extract audio from video
2. Feed to GenSlide
3. Generate slides
4. Compare with ground truth

**Metrics:**
- Content Fidelity (PresentEval)
- Visual Clarity (PresentEval)
- Structural similarity
- Generation speed

**Expected Results:**
```
Metric              | PASS  | GenSlide | Gap
--------------------|-------|----------|-----
Content Fidelity    | 9.0   | 8.2      | -0.8
Visual Clarity      | 8.5   | 7.8      | -0.7
Speed (slides/min)  | 2.0   | 6.0      | +4.0
Latency (per slide) | 30s   | 10s      | -20s
```

Acceptable trade-off: -10% quality cho 3x speed

---

### 4.2. ÄÃ¡nh giÃ¡ Ä‘á»‹nh tÃ­nh (Qualitative Evaluation)

#### 4.2.1. User Study - Lab Experiment

**Design:** Within-subjects (n=20)

**Participants:**
- 10 students
- 10 professionals
- Inclusion: Fluent Vietnamese, presentation experience

**Conditions:**
- **A**: Manual PowerPoint (baseline)
- **B**: GenSlide (our system)
- **C**: Gamma.ai (AI baseline)

**Tasks:**
Má»—i participant táº¡o 3 presentations:

1. **Task 1**: Educational - "Giáº£i thÃ­ch khÃ¡i niá»‡m khoa há»c"
2. **Task 2**: Business - "Pitch Ã½ tÆ°á»Ÿng startup"
3. **Task 3**: Personal - "Chia sáº» sá»Ÿ thÃ­ch/du lá»‹ch"

**Measurements:**

**Objective:**
- Time to completion
- Number of slides
- Errors/revisions

**Subjective:**
- **SUS (System Usability Scale)**: 0-100
- **NASA-TLX**: Cognitive load
- **Satisfaction**: Likert 1-7
  - "I enjoyed using this tool"
  - "The system felt responsive"
  - "I would use this for real work"

**Statistical Analysis:**
- Repeated measures ANOVA
- Post-hoc: Bonferroni correction
- Î± = 0.05

**Hypotheses:**

H1: GenSlide faster than PowerPoint
- Expected: p < 0.001, Cohen's d > 2.0

H2: GenSlide higher SUS than Gamma.ai
- Expected: p < 0.05, Î” > 10 points

H3: GenSlide lower NASA-TLX than Manual
- Expected: p < 0.01

---

#### 4.2.2. Expert Review

**Participants:** n=3 experts (5+ years experience)

**Materials:** 50 slides from GenSlide

**Evaluation:**
- Content quality
- Visual design
- Professional appearance

**Rating Scale:** 0-10 each

**Inter-rater Reliability:** Fleiss' Kappa > 0.6

---

### 4.3. Ablation Studies

#### Experiment 1: Streaming vs Batch

**Conditions:**
- A: Full streaming
- B: Batch processing
- C: Streaming + Polish

**Expected:**
- A: Fastest (TTFC < 2s), quality 7.8/10
- B: Slowest (TTFC 5s), quality 8.2/10
- C: Fast (TTFC < 2s), quality 8.2/10 â† Best!

**Contribution:** Polish pass eliminates gap

---

#### Experiment 2: Update Frequency

**Conditions:** 1Hz, 3Hz, 6Hz, 10Hz, 15Hz, 30Hz

**Expected:** Optimal at 8-10 Hz (inverted U-shape)

**Contribution:** Evidence-based UI guideline

---

#### Experiment 3: Two-Pass vs Single-Pass ASR

**Conditions:**
- A: Two-pass (CTC + Attention)
- B: CTC only
- C: Attention only

**Expected:** A = best balance

---

## 5. Káº¾T QUáº¢ Dá»° KIáº¾N

### 5.1. Deliverables

**Technical:**
1. âœ… GenSlide Web Application
2. âœ… Source code (GitHub, open-source)
3. âœ… Docker containers
4. âœ… API documentation
5. âœ… User manual (Vietnamese + English)

**Research:**
1. âœ… Research paper (8-10 pages)
   - Target: ACL, EMNLP, CHI
2. âœ… Experiment data + scripts
3. âœ… Benchmark results (SlideSpeech)
4. âœ… User study data
5. âœ… Demo video (3-5 min)

**Dataset:**
1. âœ… Vietnamese speech-to-slide test set
2. âœ… Benchmark protocol
3. âœ… Baseline results

---

### 5.2. Expected Performance

| Metric | Target | Baseline | Improvement |
|--------|--------|----------|-------------|
| Time per slide | < 10s | 3-5 min | 18-30x |
| TTFC | < 2s | N/A | Instant |
| Content Fidelity | > 8.0/10 | 10/10 | -20% OK |
| Visual Clarity | > 7.5/10 | 8-9/10 | -15% OK |
| SUS Score | > 75 | - | Good |

**Qualitative:**
- "Fast", "Instant", "Magical"
- "Helps me focus on content"
- "Natural interaction"

---

### 5.3. Research Contributions

**Novel Contributions:**

1. **C1: Streaming Architecture**
   - First real-time speech-to-slide system
   - 2-3x faster TTFC

2. **C2: Incremental JSON Parsing**
   - State machine parser for partial JSON
   - Open-source library

3. **C3: Dual-Mode Voice Interaction**
   - Brainstorm â†” Edit switching
   - Voice-only editing

4. **C4: Speech-to-Slide Benchmark**
   - First use of SlideSpeech for generation
   - Evaluation protocol

5. **C5: Optimal Update Frequency**
   - Evidence: 6-10 Hz optimal
   - Generalizable guidelines

**Empirical Findings:**
- E1: Streaming maintains quality with polish
- E2: Incremental DOM 10x more responsive
- E3: Two-pass ASR optimal
- E4: Multi-LLM routing improves balance

**Practical Impact:**
- Teachers: Faster lecture prep
- Business: Visual brainstorming
- Researchers: Rapid prototyping
- Accessibility: Voice-first authoring

---

## 6. TIMELINE & MILESTONES

### Phase 1: Foundation (Weeks 1-4)

**Week 1: Setup & ASR**
- [ ] Project structure, Git
- [ ] WhisperX installation
- [ ] VAD implementation
- **Milestone:** Audio â†’ Text

**Week 2: Streaming ASR**
- [ ] Two-pass decoding
- [ ] Sliding window
- [ ] Word timestamps
- **Milestone:** Streaming ASR < 2s

**Week 3: LLM Integration**
- [ ] Gemini/GPT APIs
- [ ] Streaming calls
- [ ] Incremental parser
- **Milestone:** Text â†’ JSON streaming

**Week 4: Basic Rendering**
- [ ] Streamlit + React
- [ ] Incremental DOM
- [ ] Animations
- **Milestone:** End-to-end MVP

---

### Phase 2: Advanced Features (Weeks 5-6)

**Week 5: Dual-Mode**
- [ ] State Manager
- [ ] Brainstorm/Edit prompts
- [ ] Mode switching UI
- **Milestone:** Mode switching works

**Week 6: Context & Commands**
- [ ] Command parsing
- [ ] Reference resolution
- [ ] Complex edits
- **Milestone:** Edit mode functional

---

### Phase 3: Optimization (Weeks 7-8)

**Week 7: Performance**
- [ ] Profile latency
- [ ] Optimize DOM (60 FPS)
- [ ] Multi-LLM routing
- **Milestone:** TTFC < 2s

**Week 8: Quality**
- [ ] Polish pass
- [ ] Error handling
- [ ] Edge cases
- **Milestone:** Stable system

---

### Phase 4: Evaluation (Weeks 9-11)

**Week 9: Benchmark**
- [ ] SlideSpeech test set
- [ ] Automated evaluation
- [ ] PresentEval scores
- **Milestone:** Quantitative results

**Week 10-11: User Study**
- [ ] Recruit n=20
- [ ] Lab experiments
- [ ] Questionnaires
- [ ] Statistical analysis
- **Milestone:** User study complete

---

### Phase 5: Writing (Week 12)

**Week 12: Documentation**
- [ ] Research paper (8-10 pages)
- [ ] Demo video
- [ ] Defense slides
- [ ] GitHub README
- **Milestone:** All deliverables ready

---

## 7. Rá»¦I RO & GIáº¢I PHÃP

### Risk 1: Latency > 2s

**Likelihood:** Medium  
**Impact:** High

**Mitigation:**
- Optimize from Week 1
- Use Gemini Flash (fastest)
- Caching common phrases
- GPU acceleration

**Fallback:**
- Accept 3s latency
- Focus on streaming UI feel

---

### Risk 2: Intent Accuracy < 85%

**Likelihood:** Medium  
**Impact:** Medium

**Mitigation:**
- Use GPT-4o for reasoning
- Confirmation prompts
- Learn from corrections

**Fallback:**
- Manual mode selection
- Focus on common intents

---

### Risk 3: API Costs Too High

**Likelihood:** Low-Medium  
**Impact:** Medium

**Mitigation:**
- Gemini Flash primary
- Cache responses
- Batch when possible

**Fallback:**
- Local LLM (LLaMA 3.1 8B)
- Reduce quality slightly

---

### Risk 4: User Recruitment Fails

**Likelihood:** Low  
**Impact:** Medium

**Mitigation:**
- Start Week 8
- Incentives
- University lists

**Fallback:**
- Online study (Prolific)
- Accept n=10

---

### Risk 5: Vietnamese ASR Poor

**Likelihood:** Medium  
**Impact:** High

**Mitigation:**
- Test early (Week 1)
- Fine-tune if needed
- Word-level confidence

**Fallback:**
- Accept 10-15% WER
- Manual correction mode

---

## 8. ÄÃ“NG GÃ“P KHOA Há»ŒC

### 8.1. LÃ½ thuyáº¿t (Theoretical)

**T1: Streaming Architecture Framework**
- Latency vs Quality vs Cost analysis
- Design patterns for real-time AI

**T2: Dual-Mode Interaction Model**
- Formal model for voice modes
- Context management

**T3: Incremental Structured Generation**
- Partial JSON parsing theory
- Renderability conditions

---

### 8.2. Thá»±c nghiá»‡m (Empirical)

**E1: Speech-to-Slide Benchmark**
- First SlideSpeech generation use
- Protocol documentation

**E2: Optimal Update Frequency**
- Evidence: 6-10 Hz
- Inverted U-shape confirmed

**E3: Vietnamese ASR Benchmarks**
- Presentation domain WER
- Best practices

**E4: User Preference Study**
- Real-time vs Batch data
- Cognitive load comparison

---

### 8.3. Ká»¹ thuáº­t (Technical)

**Tech1: Open-source**
- GenSlide codebase
- Reusable components:
  - StreamingASR
  - IncrementalJSONParser
  - StateManager

**Tech2: Reproducible**
- Docker containers
- Evaluation scripts
- Dataset tools

**Tech3: Documentation**
- API docs
- User manual
- Developer guide

---

## 9. Háº N CHáº¾ & HÆ¯á»šNG PHÃT TRIá»‚N

### 9.1. Háº¡n cháº¿

**L1: Language**
- Current: Vietnamese only
- Future: Multi-language

**L2: Content Types**
- Current: Text + placeholders
- Future: Image gen, charts

**L3: Personalization**
- Current: Default templates
- Future: Learn user style

**L4: Collaboration**
- Current: Single user
- Future: Multi-speaker

**L5: Output Formats**
- Current: Web display
- Future: PPTX, PDF export

---

### 9.2. HÆ°á»›ng phÃ¡t triá»ƒn

**FR1: Advanced Personalization (6-12 months)**
- Few-shot style learning
- Layout preferences
- Color/font extraction

**FR2: Multimodal Input (12 months)**
- Gesture recognition
- Whiteboard integration
- Screen share annotation

**FR3: Interactive Presentation (12 months)**
- Voice commands during delivery
- Real-time Q&A slides
- Audience polling

**FR4: Collaboration (18 months)**
- Multi-speaker diarization
- Team brainstorming
- Real-time collaboration

**FR5: Domain Adaptation**
- Medical (specialized terms)
- Legal (citations)
- Technical (code snippets)
- Business (charts)

---

## 10. Káº¾T LUáº¬N

### 10.1. TÃ³m táº¯t

Äá» tÃ i giáº£i quyáº¿t váº¥n Ä‘á»: **Táº¡o slide presentation tá»± nhiÃªn, nhanh chÃ³ng, chá»‰ báº±ng giá»ng nÃ³i**.

**ÄÃ³ng gÃ³p chÃ­nh:**

1. **True Streaming Architecture**: First system combining streaming ASR + LLM + incremental rendering
2. **Dual-Mode Voice**: Brainstorm â†” Edit chá»‰ báº±ng giá»ng nÃ³i
3. **Incremental JSON Parsing**: Parse vÃ  render khi JSON Ä‘ang generate
4. **Speech-to-Slide Benchmark**: First SlideSpeech generation use
5. **Evidence-based Guidelines**: Optimal 6-10 Hz update frequency

**Impact:**

- **Academic**: Novel direction, reproducible, open-source
- **Practical**: 18-30x faster, better brainstorming, accessibility
- **Industry**: Foundation for voice productivity tools

---

### 10.2. TÃ­nh kháº£ thi

**Technical: HIGH**
- All components proven
- Tools available
- No fundamental barriers

**Research: HIGH**
- Clear questions + metrics
- Established frameworks
- Available dataset
- Realistic 12-week timeline

**Resource: MEDIUM-HIGH**
- API costs: ~$50-100
- CPU sufficient (GPU optional)
- University students available
- Advisor support needed

---

### 10.3. TÃ¡c Ä‘á»™ng

**Short-term (6 months):**
- Functional prototype
- Conference submission
- Open-source release
- Community feedback

**Medium-term (1-2 years):**
- Educational adoption
- Tool integrations (Zoom, Teams)
- Follow-up research
- Improved versions

**Long-term (3+ years):**
- Standard voice-driven tool
- Foundation for productivity apps
- Presentation software influence
- Accessibility impact

---

## 11. TÃ€I LIá»†U THAM KHáº¢O

### Academic Papers

1. **Liu, S., et al. (2025).** "PASS: Presentation Automation for Slide Generation and Speech". *arXiv:2501.06497*.

2. **Zhang, L., et al. (2025).** "PresentAgent: Multimodal Agent for Presentation Video Generation". *arXiv:2507.04036*.

3. **Li, J., et al. (2025).** "Adapting Whisper for Streaming Speech Recognition via Two-Pass Decoding". *arXiv:2506.12154*.

4. **Bain, M., et al. (2023).** "WhisperX: Time-Accurate Speech Transcription". *GitHub: m-bain/whisperX*.

5. **Xu, M., et al. (2023).** "SlideSpeech: Large-Scale Slide-Enriched Corpus". *OpenSLR 144*.

6. **Krishnan, G., et al. (2022).** "Few-shot Style Transfer for Multilingual Settings". *arXiv:2110.07385*.

7. **Huang, Y., et al. (2022).** "LayoutLMv3: Document AI Pre-training". *ACM MM 2022*.

8. **Li, Z., et al. (2024).** "PosterLLaVa: Layout Generator with LLM". *arXiv:2406.02884*.

9. **Chen, X., et al. (2023).** "Auto-Slides: Multi-Agent Collaboration". *GitHub: Westlake-AGI-Lab/Auto-Slides*.

---

## PHá»¤ Lá»¤C

### Phá»¥ lá»¥c A: System Prompts

**Brainstorm Mode:**
```
Báº¡n lÃ  trá»£ lÃ½ GenSlide. User Ä‘ang brainstorm.

Output JSON:
{
  "intent": "create_slide" | "add_content",
  "content": {
    "title": "...",
    "bullets": ["...", "..."]
  }
}

Rules:
- Title: 5-10 tá»«
- Bullets: Max 5, má»—i bullet 10-15 tá»«
- Ngáº¯n gá»n, sÃºc tÃ­ch
```

**Edit Mode:**
```
Báº¡n lÃ  trá»£ lÃ½ edit. User ra Lá»†NH.

Output JSON:
{
  "command": "edit_title" | "delete_bullet" | "add_bullet",
  "target": {...},
  "new_value": "..."
}

Context:
- "cÃ¡i Ä‘Ã³" = last mentioned
- "slide nÃ y" = current
```

---

### Phá»¥ lá»¥c B: Evaluation Rubrics

**Content Fidelity:**
```
9-10: All key points accurate
7-8:  Most points, minor omissions
5-6:  Some missing/inaccurate
3-4:  Significant issues
1-2:  Major inaccuracies
```

**Visual Clarity:**
```
9-10: Professional, highly readable
7-8:  Good, minor issues
5-6:  Acceptable but issues
3-4:  Poor layout
1-2:  Very poor
```

---

### Phá»¥ lá»¥c C: User Study Materials

**Task Instructions:**
```
Task 1: 3-slide presentation "Photosynthesis"

- Speak naturally
- Include: definition, process, importance
- Complete in 10 minutes
```

**Questionnaires:**
- SUS (10 questions)
- NASA-TLX (6 dimensions)
- Custom satisfaction (Likert 1-7)

---

### Phá»¥ lá»¥c D: Code Examples

**Incremental Parser:**
```python
class IncrementalJSONParser:
    def feed(self, token):
        self.buffer += token
        
        if self.state == "IN_TITLE" and '"' in token:
            title = self.extract_title()
            self.result['title'] = title
            return (True, self.result)
        
        return (False, None)
```

**Virtual DOM Differ:**
```javascript
function computeDiff(oldState, newState) {
    return {
        titleChanged: oldState.title !== newState.title,
        titleDelta: newState.title.slice(oldState.title.length),
        bulletsAdded: newState.bullets.slice(oldState.bullets.length)
    };
}
```

---

## CÃ‚U Há»I THÆ¯á»œNG Gáº¶P

**Q1: Táº¡i sao khÃ´ng dÃ¹ng keyboard cho editing?**  
A: Äá»ƒ maintain flow state - focus 100% vÃ o content.

**Q2: Latency < 2s cÃ³ thá»±c táº¿ khÃ´ng?**  
A: CÃ³. U2 Whisper: 300ms + 1.5s. Gemini: 500ms. Total ~2s.

**Q3: Cháº¥t lÆ°á»£ng cÃ³ tá»‘t nhÆ° human khÃ´ng?**  
A: KhÃ´ng (8/10 vs 10/10). NhÆ°ng trade-off OK: -20% quality cho 30x speed.

**Q4: Support tiáº¿ng Viá»‡t khÃ´ng?**  
A: CÃ³. WhisperX + Gemini/GPT support Vietnamese.

**Q5: Chi phÃ­?**  
A: Development: ~$50-100. Production: ~$0.01-0.05/slide.

---

**Káº¾T THÃšC Äá»€ CÆ¯Æ NG**

---